{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "data = fetch_openml(name=\"boston\", version=1, as_frame=False)\n",
    "X, y = data.data, data.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'D:\\Maths Y4\\Project\\TensorBNN')\n",
    "\n",
    "# Now you can import TensorBNN or any module within it\n",
    "from tensorBNN.layer import DenseLayer\n",
    "from tensorBNN.network import network\n",
    "from tensorBNN.activationFunctions import Relu\n",
    "from tensorBNN.likelihood import GaussianLikelihood\n",
    "from tensorBNN.metrics import SquaredError\n",
    "from tensorBNN.metrics import PercentError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_me_data():\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tqdm import tqdm\n",
    "    def unnormalised_ground_truth(x):\n",
    "        \"\"\"return the 'ground truth' output for a specific input location x\"\"\"\n",
    "        return 15*np.cos(0.1/(0.05*(x+1)+0.02)) + 3*np.exp(0.5*np.sin(((50*(x+1))**0.9-3)/5))/((50*(x+1)+0.01)**(-0.5)) -12.5*(x+1)**2\n",
    "\n",
    "    def normalised_ground_truth(xs):\n",
    "        ys = unnormalised_ground_truth(xs)\n",
    "        m = ys.mean()\n",
    "        s = ys.std()\n",
    "        return (ys - m) / s\n",
    "\n",
    "    xs = np.linspace(-1, 1, 1000)\n",
    "    ys = normalised_ground_truth(xs)\n",
    "\n",
    "    def generate_dataset(size=50, noise=0.15, split=0.3):\n",
    "        xs = np.linspace(-1, 1, 1000)\n",
    "        ys = normalised_ground_truth(xs)\n",
    "        samp_ind = np.random.randint(0, 999, size)\n",
    "        x_samps = xs[samp_ind]\n",
    "        y_samps = ys[samp_ind] + np.random.normal(0, noise, size)\n",
    "        return train_test_split(x_samps, y_samps, test_size=split, random_state=13)\n",
    "\n",
    "    x_tr, x_te, y_tr, y_te = generate_dataset()\n",
    "    # x_tr = torch.FloatTensor(x_tr).unsqueeze(-1)\n",
    "    # y_tr = torch.FloatTensor(y_tr).unsqueeze(-1)\n",
    "    # x_te = torch.FloatTensor(x_te).unsqueeze(-1)\n",
    "    # y_te = torch.FloatTensor(y_te).unsqueeze(-1) \n",
    "\n",
    "    # Convert NumPy arrays to TensorFlow tensors\n",
    "    x_tr_tf = tf.convert_to_tensor(x_tr, dtype=tf.float32)\n",
    "    y_tr_tf = tf.convert_to_tensor(y_tr, dtype=tf.float32)\n",
    "    x_te_tf = tf.convert_to_tensor(x_te, dtype=tf.float32)\n",
    "    y_te_tf = tf.convert_to_tensor(y_te, dtype=tf.float32)\n",
    "   \n",
    "    return x_tr_tf, y_tr_tf, x_te_tf, y_te_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr,y_tr,x_te,y_te = give_me_data()\n",
    "y_tr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralNet = network(dtype=tf.float32, inputDims=1, trainX=x_tr,trainY= y_tr,validateX= x_te,validateY= y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralNet.add(DenseLayer(1, 2, dtype=tf.float32))\n",
    "neuralNet.add(Relu())\n",
    "neuralNet.add(DenseLayer(2, 1, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralNet.setupMCMC(stepSizeStart=1e-3, stepSizeMin=1e-4, stepSizeMax=1e-2,\n",
    "                  stepSizeOptions=40, leapfrogStart=1000, leapfogMin=100,\n",
    "                  leapFrogMax=10000, leapfrogIncrement=1, hyperStepSize=1e-2,\n",
    "                  hyperLeapfrog=100, burnin=1000,\n",
    "                  cores=4, averagingSteps=10, a=4, delta=0.1, strikes=5,\n",
    "                  randomSteps=10, dualAveraging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuralNet.burnin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Gaussian Likelihood with sd of 0.1\n",
    "likelihood =  GaussianLikelihood(sd = 0.1)\n",
    "metricList = [ #Declare metrics\n",
    "    SquaredError(mean = 0, sd = 1, scaleExp = False),\n",
    "    PercentError(mean = 10, sd = 2, scaleExp = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter: 1\n",
      "step size 0.001\n",
      "hyper step size 0.80580854\n",
      "leapfrog 1000\n",
      "Main acceptance 1.1883829e-05\n",
      "Hyper acceptance 0.0\n",
      "training squared error  3.99994 validation squared error  2.27408\n",
      "training percent error 67739.609 validation percent error 8451.083\n",
      "Loss Standard Deviation:  0.1\n",
      "Time elapsed: 9.896152973175049\n",
      "\n",
      "iter: 2\n",
      "step size 0.001\n",
      "hyper step size 0.65679485\n",
      "leapfrog 1000\n",
      "Main acceptance 3.0238735e-19\n",
      "Hyper acceptance 0.0\n",
      "training squared error  3.99994 validation squared error  2.27408\n",
      "training percent error 67739.609 validation percent error 8451.083\n",
      "SJD: 0.0\n",
      "Loss Standard Deviation:  0.1\n",
      "Time elapsed: 9.428685426712036\n",
      "\n",
      "iter: 3\n",
      "step size 0.001\n",
      "hyper step size 0.5207874\n",
      "leapfrog 1000\n",
      "Main acceptance 9.474614e-06\n",
      "Hyper acceptance 0.0\n",
      "training squared error  3.99994 validation squared error  2.27408\n",
      "training percent error 67739.609 validation percent error 8451.083\n",
      "SJD: 0.0\n",
      "Loss Standard Deviation:  0.1\n",
      "Time elapsed: 0.016996145248413086\n",
      "\n",
      "iter: 4\n",
      "step size 0.001\n",
      "hyper step size 0.4059307\n",
      "leapfrog 1000\n",
      "Main acceptance 0.008974402\n",
      "Hyper acceptance 0.0\n",
      "training squared error  3.99994 validation squared error  2.27408\n",
      "training percent error 67739.609 validation percent error 8451.083\n",
      "SJD: 0.0\n",
      "Loss Standard Deviation:  0.1\n",
      "Time elapsed: 0.017000198364257812\n",
      "\n",
      "iter: 5\n",
      "step size 0.001\n",
      "hyper step size 0.31306037\n",
      "leapfrog 1000\n",
      "Main acceptance 1.8729875e-19\n",
      "Hyper acceptance 0.0\n",
      "training squared error  3.99994 validation squared error  2.27408\n",
      "training percent error 67739.609 validation percent error 8451.083\n",
      "SJD: 0.0\n",
      "Loss Standard Deviation:  0.1\n",
      "Time elapsed: 0.015999555587768555\n",
      "\n",
      "iter: 6\n",
      "step size 0.001\n",
      "hyper step size 0.23991266\n",
      "leapfrog 1000\n",
      "Main acceptance 2.5059984e-05\n",
      "Hyper acceptance 0.0\n",
      "training squared error  3.99994 validation squared error  2.27408\n",
      "training percent error 67739.609 validation percent error 8451.083\n",
      "SJD: 0.0\n",
      "Loss Standard Deviation:  0.1\n",
      "Time elapsed: 0.016294240951538086\n",
      "\n",
      "iter: 7\n",
      "step size 0.001\n",
      "hyper step size 0.18323128\n",
      "leapfrog 1000\n",
      "Main acceptance 1.1348696e-16\n",
      "Hyper acceptance 0.0\n",
      "training squared error  3.99994 validation squared error  2.27408\n",
      "training percent error 67739.609 validation percent error 8451.083\n",
      "SJD: 0.0\n",
      "Loss Standard Deviation:  0.1\n",
      "Time elapsed: 0.015999794006347656\n",
      "\n",
      "iter: 8\n",
      "step size 0.001\n",
      "hyper step size 0.13975118\n",
      "leapfrog 1000\n",
      "Main acceptance 0.038098585\n",
      "Hyper acceptance 0.0\n",
      "training squared error  1.17644 validation squared error  1.09684\n",
      "training percent error 509.251 validation percent error 211.282\n",
      "SJD: 268.33084\n",
      "Loss Standard Deviation:  0.1\n",
      "Time elapsed: 0.016000032424926758\n",
      "\n",
      "iter: 9\n",
      "step size 0.001\n",
      "hyper step size 0.106598064\n",
      "leapfrog 1000\n",
      "Main acceptance 1.0\n",
      "Hyper acceptance 0.0\n",
      "training squared error  0.98101 validation squared error  1.03815\n",
      "training percent error 396.501 validation percent error 238.935\n",
      "SJD: 0.9019984\n",
      "Loss Standard Deviation:  0.1\n",
      "Time elapsed: 0.017000913619995117\n",
      "\n",
      "iter:10\n",
      "step size 0.001\n",
      "hyper step size 0.08140094\n",
      "leapfrog 1000\n",
      "Main acceptance 4.324445e-06\n",
      "Hyper acceptance 0.0\n",
      "training squared error  0.98101 validation squared error  1.03815\n",
      "training percent error 396.501 validation percent error 238.935\n",
      "SJD: 0.0\n",
      "Loss Standard Deviation:  0.1\n",
      "Time elapsed: 0.015640735626220703\n",
      "\n",
      "iter:11\n",
      "step size 0.001\n",
      "hyper step size 0.06227531\n",
      "leapfrog 1000\n",
      "Main acceptance 9.780112e-05\n",
      "Hyper acceptance 0.0\n",
      "training squared error  0.98101 validation squared error  1.03815\n",
      "training percent error 396.501 validation percent error 238.935\n",
      "SJD: 0.0\n",
      "Loss Standard Deviation:  0.1\n",
      "Time elapsed: 0.015999794006347656\n",
      "\n",
      "iter:12\n",
      "step size 0.001\n",
      "hyper step size 0.047757026\n",
      "leapfrog 1000\n",
      "Main acceptance 0.6066417\n",
      "Hyper acceptance 0.0\n",
      "training squared error  0.69697 validation squared error  0.85300\n",
      "training percent error 252.600 validation percent error 408.745\n",
      "SJD: 3.069657\n",
      "Loss Standard Deviation:  0.1\n",
      "Time elapsed: 0.01699995994567871\n",
      "\n",
      "iter:13\n",
      "step size 0.001\n",
      "hyper step size 0.036724366\n",
      "leapfrog 1000\n",
      "Main acceptance 1.0\n",
      "Hyper acceptance 0.0\n",
      "training squared error  0.66522 validation squared error  0.73776\n",
      "training percent error 118.904 validation percent error 146.741\n",
      "SJD: 0.4704639\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mneuralNet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# epochs to train for\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43msamplingStep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# increment between network saves\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetricList\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetricList\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfolderName\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRegression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Name of folder for saved networks\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnetworksPerFile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# Number of networks saved per file\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Maths Y4\\Project\\TensorBNN\\tensorBNN\\network.py:603\u001b[0m, in \u001b[0;36mnetwork.train\u001b[1;34m(self, epochs, samplingStep, likelihood, metricList, adjustHypers, scaleExp, folderName, networksPerFile, displaySkip)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyper acceptance\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperAccept\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainY,\n\u001b[0;32m    602\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidateY)\n\u001b[1;32m--> 603\u001b[0m step, leap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_size \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleapfrog \u001b[38;5;241m=\u001b[39m leap \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleapfrog \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mD:\\Maths Y4\\Project\\TensorBNN\\tensorBNN\\paramAdapter.py:283\u001b[0m, in \u001b[0;36mparamAdapter.update\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgridSearch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreviousGamma, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverseR, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrootbeta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meu, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentE \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meGrid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentL \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlGrid)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(size\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m50\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\random.py:369\u001b[0m, in \u001b[0;36mRandom.choice\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchoice\u001b[39m(\u001b[38;5;28mself\u001b[39m, seq):\n\u001b[0;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m seq:\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot choose from an empty sequence\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m seq[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow(\u001b[38;5;28mlen\u001b[39m(seq))]\n",
      "File \u001b[1;32mc:\\Users\\peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:321\u001b[0m, in \u001b[0;36m_EagerTensorBase.__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m--> 321\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numpy())\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "neuralNet.train(\n",
    "        epochs= 1000, # epochs to train for\n",
    "        samplingStep= 5, # increment between network saves\n",
    "        likelihood=likelihood,\n",
    "        metricList = metricList,\n",
    "        folderName = \"Regression\", \n",
    "        # Name of folder for saved networks\n",
    "        networksPerFile=50)\n",
    "        # Number of networks saved per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TensorBNN.predictor import predictor \n",
    "\n",
    "network = predictor(filePath,\n",
    "                    dtype = tf.float32, \n",
    "                    # data type used by network\n",
    "                    customLayerDict={\"dense2\": Dense2},\n",
    "                    # A dense layer with a different \n",
    "                    # hyperprior\n",
    "                    likelihood = likelihood)\n",
    "                    # The likelihood function is required to  \n",
    "                    # calculate the probabilities for \n",
    "                    # re-weighting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
