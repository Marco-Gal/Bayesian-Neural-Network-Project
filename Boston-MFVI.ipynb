{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanFieldLayer(nn.Module):\n",
    "    \"\"\"Represents a mean-field Gaussian distribution over each layer of the network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, init_var=1e-3):\n",
    "        super(MeanFieldLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Prior parameters p(theta)\n",
    "        self.w_mu_p = torch.zeros(input_dim, output_dim)\n",
    "        self.w_log_var_p = torch.zeros(input_dim, output_dim)\n",
    "        self.b_mu_p = torch.zeros(output_dim)\n",
    "        self.b_log_var_p = torch.zeros(output_dim)\n",
    "\n",
    "        # Variational parameters q(theta)\n",
    "        self.w_mu_q = nn.Parameter(torch.zeros(input_dim, output_dim), requires_grad=True)\n",
    "        self.w_log_var_q = nn.Parameter(\n",
    "            torch.ones(input_dim, output_dim) * torch.log(torch.tensor(init_var)), requires_grad=True\n",
    "        )  \n",
    "        self.b_mu_q = nn.Parameter(torch.zeros(output_dim), requires_grad=True)\n",
    "        self.b_log_var_q = nn.Parameter(\n",
    "            torch.ones(output_dim) * torch.log(torch.tensor(init_var)), requires_grad=True\n",
    "        )\n",
    "\n",
    "    # the priors do not change so could be stored as attributes, but\n",
    "    # it feels cleaner to access them in the same way as the posteriors\n",
    "    def p_w(self):\n",
    "        \"\"\"weight prior distribution\"\"\"\n",
    "        return torch.distributions.Normal(self.w_mu_p, (0.5 * self.w_log_var_p).exp())\n",
    "\n",
    "    def p_b(self):\n",
    "        \"\"\"bias prior distribution\"\"\"\n",
    "        return torch.distributions.Normal(self.b_mu_p, (0.5 * self.b_log_var_p).exp())\n",
    "\n",
    "    def q_w(self):\n",
    "        \"\"\"variational weight posterior\"\"\"\n",
    "        return torch.distributions.Normal(self.w_mu_q, (0.5 * self.w_log_var_q).exp())\n",
    "\n",
    "    def q_b(self):\n",
    "        \"\"\"variational bias posterior\"\"\"\n",
    "        return torch.distributions.Normal(self.b_mu_q, (0.5 * self.b_log_var_q).exp())\n",
    "\n",
    "    def kl(self):\n",
    "        weight_kl = torch.distributions.kl.kl_divergence(self.q_w(), self.p_w()).sum() \n",
    "        bias_kl = torch.distributions.kl.kl_divergence(self.q_b(), self.p_b()).sum()\n",
    "        return weight_kl + bias_kl\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Propagates x through this layer by sampling weights from the posterior\"\"\"\n",
    "        assert (len(x.shape) == 3), \"x should be shape (num_samples, batch_size, input_dim).\"\n",
    "        assert x.shape[-1] == self.input_dim\n",
    "\n",
    "        num_samples = x.shape[0]\n",
    "        # rsample carries out reparameterisation trick for us\n",
    "        weights = self.q_w().rsample((num_samples,))  # (num_samples, input_dim, output_dim).\n",
    "        biases = self.q_b().rsample((num_samples,)).unsqueeze(1)  # (num_samples, batch_size, output_dim)\n",
    "        return x @ weights + biases # (num_samples, batch_size, output_dim).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MeanFieldBNN(nn.Module):\n",
    "    \"\"\"Mean-field variational inference BNN.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dims,\n",
    "        output_dim,\n",
    "        activation=nn.ELU(),\n",
    "        noise_std=1.0,\n",
    "    ):\n",
    "        super(MeanFieldBNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.log_noise_var = torch.log(torch.tensor(noise_std**2))\n",
    "\n",
    "        self.network = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) + 1):\n",
    "            if i == 0:\n",
    "                self.network.append(MeanFieldLayer(self.input_dim, self.hidden_dims[i]))\n",
    "                self.network.append(self.activation)\n",
    "            elif i == len(hidden_dims):\n",
    "                self.network.append(\n",
    "                    MeanFieldLayer(self.hidden_dims[i - 1], self.output_dim)\n",
    "                )\n",
    "            else:\n",
    "                self.network.append(\n",
    "                    MeanFieldLayer(self.hidden_dims[i - 1], self.hidden_dims[i])\n",
    "                )\n",
    "                self.network.append(self.activation) \n",
    "\n",
    "    def forward(self, x, num_samples=1):\n",
    "        \"\"\"Propagate the inputs through the network using num_samples weights.\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): Inputs to the network.\n",
    "            num_samples (int, optional): Number of samples to use. Defaults to 1.\n",
    "        \"\"\"\n",
    "        assert len(x.shape) == 2, \"x.shape must be (batch_size, input_dim).\"\n",
    "\n",
    "        # Expand dimensions of x to (num_samples, batch_size, input_dim).\n",
    "        x = torch.unsqueeze(x, 0).repeat(num_samples, 1, 1)\n",
    "\n",
    "        # Propagate x through network\n",
    "        for layer in self.network:\n",
    "            x = layer(x)\n",
    "\n",
    "        assert len(x.shape) == 3, \"x.shape must be (num_samples, batch_size, output_dim)\"\n",
    "        assert x.shape[-1] == self.output_dim\n",
    "\n",
    "        return x\n",
    "\n",
    "    def ll(self, y_obs, y_pred, num_samples=1):\n",
    "        \"\"\"Computes the log likelihood of the outputs of self.forward(x)\"\"\"\n",
    "        l = torch.distributions.normal.Normal(y_pred, torch.sqrt(torch.exp(self.log_noise_var)))\n",
    "        \n",
    "        # take mean over num_samples dim, sum over batch_size dim\n",
    "        # note that after taking mean, batch_size becomes dim 0\n",
    "        return l.log_prob(y_obs.unsqueeze(0).repeat(num_samples, 1, 1)).mean(0).sum(0).squeeze()\n",
    "\n",
    "    def kl(self):\n",
    "        \"\"\"Computes the KL divergence between the approximate posterior and the prior for the network.\"\"\"\n",
    "        return sum([layer.kl() for layer in self.network if isinstance(layer, MeanFieldLayer)])\n",
    "\n",
    "    def loss(self, x, y, num_samples=1):\n",
    "        \"\"\"Computes the ELBO and returns its negative\"\"\"\n",
    "\n",
    "        y_pred = self.forward(x, num_samples=num_samples)\n",
    "        \n",
    "        exp_ll = self.ll(y, y_pred, num_samples=num_samples)\n",
    "        kl = self.kl()\n",
    "\n",
    "        return kl - exp_ll, exp_ll, kl"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
