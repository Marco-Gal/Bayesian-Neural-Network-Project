{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\peter\\.cache\\kagglehub\\datasets\\brendan45774\\wine-quality\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"brendan45774/wine-quality\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ['winequality-red.csv', 'winequality-white.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the downloaded dataset\n",
    "path = r\"C:\\Users\\peter\\.cache\\kagglehub\\datasets\\brendan45774\\wine-quality\\versions\\2\"\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(path)\n",
    "print(\"Files in the directory:\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file (replace with the actual file name)\n",
    "csv_file_path = os.path.join(path, \"winequality-red.csv\")\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path, sep=\";\")\n",
    "\n",
    "features = df.drop(columns=[\"quality\"])\n",
    "\n",
    "# Separate targets (only the \"quality\" column)\n",
    "targets = df[\"quality\"]\n",
    "X_tensor = torch.tensor(features.values, dtype=torch.float32)\n",
    "\n",
    "# Convert targets to a PyTorch tensor\n",
    "y_tensor = torch.tensor(targets.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = train_test_split(X_tensor, y_tensor, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanFieldLayer(nn.Module):\n",
    "    \"\"\"Represents a mean-field Gaussian distribution over each layer of the network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, init_var=5e-2, prior_var = 1):\n",
    "        super(MeanFieldLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Prior parameters p(theta)\n",
    "        self.w_mu_p = torch.zeros(input_dim, output_dim)\n",
    "        self.w_log_var_p = torch.ones(input_dim, output_dim) * torch.log(torch.tensor(prior_var))\n",
    "\n",
    "        # self.w_log_var_p = torch.zeros(input_dim, output_dim)\n",
    "        self.b_mu_p = torch.zeros(output_dim)\n",
    "        # self.b_log_var_p = torch.zeros(output_dim)\n",
    "        self.b_log_var_p = torch.ones(output_dim) * torch.log(torch.tensor(prior_var))\n",
    "\n",
    "\n",
    "        # Variational parameters q(theta)\n",
    "        self.w_mu_q = nn.Parameter(torch.zeros(input_dim, output_dim), requires_grad=True)\n",
    "        self.w_log_var_q = nn.Parameter(\n",
    "            torch.ones(input_dim, output_dim) * torch.log(torch.tensor(prior_var)), requires_grad=True\n",
    "        )  \n",
    "        self.b_mu_q = nn.Parameter(torch.zeros(output_dim), requires_grad=True)\n",
    "        self.b_log_var_q = nn.Parameter(\n",
    "            torch.ones(output_dim) * torch.log(torch.tensor(prior_var)), requires_grad=True\n",
    "        )\n",
    "\n",
    "    # the priors do not change so could be stored as attributes, but\n",
    "    # it feels cleaner to access them in the same way as the posteriors\n",
    "    def p_w(self):\n",
    "        \"\"\"weight prior distribution\"\"\"\n",
    "        return torch.distributions.Normal(self.w_mu_p, (0.5 * self.w_log_var_p).exp())\n",
    "\n",
    "    def p_b(self):\n",
    "        \"\"\"bias prior distribution\"\"\"\n",
    "        return torch.distributions.Normal(self.b_mu_p, (0.5 * self.b_log_var_p).exp())\n",
    "\n",
    "    def q_w(self):\n",
    "        \"\"\"variational weight posterior\"\"\"\n",
    "        return torch.distributions.Normal(self.w_mu_q, (0.5 * self.w_log_var_q).exp())\n",
    "\n",
    "    def q_b(self):\n",
    "        \"\"\"variational bias posterior\"\"\"\n",
    "        return torch.distributions.Normal(self.b_mu_q, (0.5 * self.b_log_var_q).exp())\n",
    "\n",
    "    def kl(self):\n",
    "        weight_kl = torch.distributions.kl.kl_divergence(self.q_w(), self.p_w()).sum() \n",
    "        bias_kl = torch.distributions.kl.kl_divergence(self.q_b(), self.p_b()).sum()\n",
    "        return weight_kl + bias_kl\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Propagates x through this layer by sampling weights from the posterior\"\"\"\n",
    "        assert (len(x.shape) == 3), \"x should be shape (num_samples, batch_size, input_dim).\"\n",
    "        assert x.shape[-1] == self.input_dim\n",
    "\n",
    "        num_samples = x.shape[0]\n",
    "        # rsample carries out reparameterisation trick for us   \n",
    "        weights = self.q_w().rsample((num_samples,))  # (num_samples, input_dim, output_dim).\n",
    "        # print('dim weights', weights.shape)\n",
    "\n",
    "        biases = self.q_b().rsample((num_samples,)).unsqueeze(1)  # (num_samples, batch_size, output_dim)\n",
    "        # print('dim biases', biases.shape)\n",
    "        # print('dim x', x.shape)\n",
    "        # print('dim x @ weights', (x @ weights).shape)\n",
    "        # print('dim x @ weights + biases', (x @ weights + biases).shape)\n",
    "        return x @ weights + biases # (num_samples, batch_size, output_dim).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MeanFieldBNN(nn.Module):\n",
    "    \"\"\"Mean-field variational inference BNN.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dims,\n",
    "        output_dim,\n",
    "        activation=nn.ELU(),\n",
    "        noise_std=1.0,\n",
    "        prior_var = 1\n",
    "    ):\n",
    "        super(MeanFieldBNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.log_noise_var = torch.log(torch.tensor(noise_std**2))\n",
    "        \n",
    "        self.network = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) + 1):\n",
    "            if i == 0:\n",
    "                self.network.append(MeanFieldLayer(self.input_dim, self.hidden_dims[i], prior_var = prior_var))\n",
    "                self.network.append(self.activation)\n",
    "            elif i == len(hidden_dims):\n",
    "                self.network.append(\n",
    "                    MeanFieldLayer(self.hidden_dims[i - 1], self.output_dim, prior_var = prior_var)\n",
    "                )\n",
    "            else:\n",
    "                self.network.append(\n",
    "                    MeanFieldLayer(self.hidden_dims[i - 1], self.hidden_dims[i], prior_var = prior_var)\n",
    "                )\n",
    "                self.network.append(self.activation) \n",
    "\n",
    "    def forward(self, x, num_samples=1):\n",
    "        \"\"\"Propagate the inputs through the network using num_samples weights.\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): Inputs to the network.\n",
    "            num_samples (int, optional): Number of samples to use. Defaults to 1.\n",
    "        \"\"\"\n",
    "        assert len(x.shape) == 2, \"x.shape must be (batch_size, input_dim).\"\n",
    "\n",
    "        # Expand dimensions of x to (num_samples, batch_size, input_dim).\n",
    "        x = torch.unsqueeze(x, 0).repeat(num_samples, 1, 1)\n",
    "        # Propagate x through network\n",
    "        for layer in self.network:\n",
    "            x = layer(x)\n",
    "\n",
    "        assert len(x.shape) == 3, \"x.shape must be (num_samples, batch_size, output_dim)\"\n",
    "        assert x.shape[-1] == self.output_dim\n",
    "\n",
    "        return x\n",
    "\n",
    "    def ll(self, y_obs, y_pred, num_samples=1):\n",
    "        \"\"\"Computes the log likelihood of the outputs of self.forward(x)\"\"\"\n",
    "        l = torch.distributions.normal.Normal(y_pred, torch.sqrt(torch.exp(self.log_noise_var)))\n",
    "        \n",
    "        # take mean over num_samples dim, sum over batch_size dim\n",
    "        # note that after taking mean, batch_size becomes dim 0\n",
    "        return l.log_prob(y_obs.unsqueeze(0).repeat(num_samples, 1, 1)).mean(0).sum(0).squeeze()\n",
    "\n",
    "    def kl(self):\n",
    "        \"\"\"Computes the KL divergence between the approximate posterior and the prior for the network.\"\"\"\n",
    "        return sum([layer.kl() for layer in self.network if isinstance(layer, MeanFieldLayer)])\n",
    "\n",
    "    def loss(self, x, y, num_samples=1):\n",
    "        \"\"\"Computes the ELBO and returns its negative\"\"\"\n",
    "\n",
    "        y_pred = self.forward(x, num_samples=num_samples)\n",
    "        \n",
    "        exp_ll = self.ll(y, y_pred, num_samples=num_samples)\n",
    "        kl = self.kl()\n",
    "\n",
    "        return kl - exp_ll, exp_ll, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeanFieldBNN(\n",
      "  (activation): ELU(alpha=1.0)\n",
      "  (network): ModuleList(\n",
      "    (0): MeanFieldLayer()\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): MeanFieldLayer()\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): MeanFieldLayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bnn_model = MeanFieldBNN(11, [10, 10], 1, noise_std=0.15, prior_var = 0.3)\n",
    "# find value of noise_std that works best by trial and error, but this is of course\n",
    "# inherently a bit contrived since we know we set input noise std to 0.15 in dataset generation\n",
    "print(bnn_model)\n",
    "\n",
    "opt = torch.optim.Adam(\n",
    "    bnn_model.parameters(),\n",
    "    lr = 1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5008644000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4000\u001b[39m)):  \u001b[38;5;66;03m# epochs\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m     l, ll, kl \u001b[38;5;241m=\u001b[39m \u001b[43mbnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     tr_loss_evo\u001b[38;5;241m.\u001b[39mappend(l\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     15\u001b[0m     tr_ll_evo\u001b[38;5;241m.\u001b[39mappend(ll\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[1;32mIn[6], line 147\u001b[0m, in \u001b[0;36mMeanFieldBNN.loss\u001b[1;34m(self, x, y, num_samples)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the ELBO and returns its negative\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x, num_samples\u001b[38;5;241m=\u001b[39mnum_samples)\n\u001b[1;32m--> 147\u001b[0m exp_ll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mll\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m kl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkl()\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kl \u001b[38;5;241m-\u001b[39m exp_ll, exp_ll, kl\n",
      "Cell \u001b[1;32mIn[6], line 136\u001b[0m, in \u001b[0;36mMeanFieldBNN.ll\u001b[1;34m(self, y_obs, y_pred, num_samples)\u001b[0m\n\u001b[0;32m    132\u001b[0m l \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mnormal\u001b[38;5;241m.\u001b[39mNormal(y_pred, torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_noise_var)))\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# take mean over num_samples dim, sum over batch_size dim\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# note that after taking mean, batch_size becomes dim 0\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_obs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32mc:\\Users\\peter\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\distributions\\normal.py:89\u001b[0m, in \u001b[0;36mNormal.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m     84\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     85\u001b[0m log_scale \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     86\u001b[0m     math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, Real) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m.\u001b[39mlog()\n\u001b[0;32m     87\u001b[0m )\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;241m-\u001b[39m((\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m var)\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;241m-\u001b[39m log_scale\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi))\n\u001b[0;32m     92\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5008644000 bytes."
     ]
    }
   ],
   "source": [
    "tr_loss_evo = []\n",
    "tr_ll_evo = []\n",
    "tr_kl_evo = []\n",
    "te_loss_evo = []\n",
    "\n",
    "tr_mse_loss_evo = []\n",
    "te_mse_loss_evo = []\n",
    "aux_loss = nn.MSELoss()\n",
    "\n",
    "for epoch in tqdm(range(4000)):  # epochs\n",
    "    opt.zero_grad()\n",
    "\n",
    "    l, ll, kl = bnn_model.loss(x_tr, y_tr,num_samples = 1000)\n",
    "    tr_loss_evo.append(l.item())\n",
    "    tr_ll_evo.append(ll.item())\n",
    "    tr_kl_evo.append(kl.item())\n",
    "    tr_mse_loss_evo.append(aux_loss(bnn_model(x_tr), y_tr.unsqueeze(0)).item())\n",
    "\n",
    "    te_loss_evo.append(bnn_model.loss(x_te, y_te)[0].item())\n",
    "    te_mse_loss_evo.append(aux_loss(bnn_model(x_te), y_te.unsqueeze(0)).item())\n",
    "\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "\n",
    "plt.plot(tr_loss_evo, label='Train dataset loss')\n",
    "plt.plot(te_loss_evo, label='Test dataset loss')\n",
    "plt.ylabel('ELBO loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(tr_ll_evo, label='Train dataset log-like evo')\n",
    "plt.ylabel('expected log likelihood')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.title('Expected Log Likelihood Evolution')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(tr_kl_evo, label='Train dataset kl evo')\n",
    "plt.ylabel('kl')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.title('KL Evolution')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(tr_mse_loss_evo, label='Train dataset')\n",
    "plt.plot(te_mse_loss_evo, label='Test dataset')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.title('MSE Evolution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
