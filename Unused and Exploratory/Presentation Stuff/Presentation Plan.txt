------- THE POINT OF THE PROJECT -------
- Are the typical assumptions placed upon BNNs for VI appropriate? 
- If we relax them, does/can performance benefit?

------- WHAT PEOPLE NEED TO UNDERSTAND TO UNDERSTAND THAT --------

--- Initial knowledge - NNs, Bayes and BNNs ---
- What is a NN?
- Bayes' - What is a posterior and why do we want to even do Bayes' for NNs? We want to encode prior knowledge, or regularise? Tbh I don't really know why.
	 - Why can't we get it for BNNs? Evidence. What is the evidence? Who is an evidence? Why can't we evidence? 

--- Variational Inference ---
- Process of VI 
	- MCMC why isn't feasible, especially for large networks.
	- Instead, we pick a simple family of approximate distributions and pick the best one from that family to approximate the posterior. 
		Note that usually the posterior isn't in any tractible family.
	- How do we measure that? This allows us to frame this as an optimisation problem using ELBO. 
		We would like to optimise using gradient descent, same as traditional backprop (which was ideally mentioned in NN section).

--- Choosing families ---
- Mean-field assumption
- Gaussians
- What to do with priors? This should be very short.


--- Methods ---
- Bayes by Backprop
	- Talk about reparameterisation trick. How can we generate differentiable samples?
	- This allows MC estimates of gradients with which we can do gradient descent to minimise -ELBO.
	- An example of how this is done with a diagonal Gaussian.
- SLANG
	- Someone else fill in.


