{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 544, 'name': 'Estimation of Obesity Levels Based On Eating Habits and Physical Condition ', 'repository_url': 'https://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition', 'data_url': 'https://archive.ics.uci.edu/static/public/544/data.csv', 'abstract': 'This dataset include data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. ', 'area': 'Health and Medicine', 'tasks': ['Classification', 'Regression', 'Clustering'], 'characteristics': ['Multivariate'], 'num_instances': 2111, 'num_features': 16, 'feature_types': ['Integer'], 'demographics': ['Gender', 'Age'], 'target_col': ['NObeyesdad'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2019, 'last_updated': 'Tue Sep 10 2024', 'dataset_doi': '10.24432/C5H31Z', 'creators': [], 'intro_paper': {'ID': 358, 'type': 'NATIVE', 'title': 'Dataset for estimation of obesity levels based on eating habits and physical condition in individuals from Colombia, Peru and Mexico', 'authors': 'Fabio Mendoza Palechor, Alexis De la Hoz Manotas', 'venue': 'Data in Brief', 'year': 2019, 'journal': None, 'DOI': '10.1016/j.dib.2019.104344', 'URL': 'https://www.semanticscholar.org/paper/35b40bacd2ffa9370885b7a3004d88995fd1d011', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'This dataset include data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. The data contains 17 attributes and 2111 records, the records are labeled with the class variable NObesity (Obesity Level), that allows classification of the data using the values of Insufficient Weight, Normal Weight, Overweight Level I, Overweight Level II, Obesity Type I, Obesity Type II and Obesity Type III. 77% of the data was generated synthetically using the Weka tool and the SMOTE filter, 23% of the data was collected directly from users through a web platform.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Read the article (https://doi.org/10.1016/j.dib.2019.104344) to see the description of the attributes.', 'citation': None}}\n",
      "                              name     role         type demographic  \\\n",
      "0                           Gender  Feature  Categorical      Gender   \n",
      "1                              Age  Feature   Continuous         Age   \n",
      "2                           Height  Feature   Continuous        None   \n",
      "3                           Weight  Feature   Continuous        None   \n",
      "4   family_history_with_overweight  Feature       Binary        None   \n",
      "5                             FAVC  Feature       Binary        None   \n",
      "6                             FCVC  Feature      Integer        None   \n",
      "7                              NCP  Feature   Continuous        None   \n",
      "8                             CAEC  Feature  Categorical        None   \n",
      "9                            SMOKE  Feature       Binary        None   \n",
      "10                            CH2O  Feature   Continuous        None   \n",
      "11                             SCC  Feature       Binary        None   \n",
      "12                             FAF  Feature   Continuous        None   \n",
      "13                             TUE  Feature      Integer        None   \n",
      "14                            CALC  Feature  Categorical        None   \n",
      "15                          MTRANS  Feature  Categorical        None   \n",
      "16                      NObeyesdad   Target  Categorical        None   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                None  None             no  \n",
      "1                                                None  None             no  \n",
      "2                                                None  None             no  \n",
      "3                                                None  None             no  \n",
      "4   Has a family member suffered or suffers from o...  None             no  \n",
      "5            Do you eat high caloric food frequently?  None             no  \n",
      "6        Do you usually eat vegetables in your meals?  None             no  \n",
      "7              How many main meals do you have daily?  None             no  \n",
      "8                  Do you eat any food between meals?  None             no  \n",
      "9                                       Do you smoke?  None             no  \n",
      "10                 How much water do you drink daily?  None             no  \n",
      "11         Do you monitor the calories you eat daily?  None             no  \n",
      "12           How often do you have physical activity?  None             no  \n",
      "13  How much time do you use technological devices...  None             no  \n",
      "14                    How often do you drink alcohol?  None             no  \n",
      "15           Which transportation do you usually use?  None             no  \n",
      "16                                      Obesity level  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# fetch dataset \n",
    "estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition = fetch_ucirepo(id=544) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition.data.features \n",
    "y = estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition.data.targets.copy() \n",
    "  \n",
    "# metadata \n",
    "print(estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "import torch.nn as nn\n",
    "\n",
    "from pyro.infer import MCMC, NUTS\n",
    "from pyro.infer import Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC','CALC','MTRANS']\n",
    "# One-hot encode categorical features, dropping the first category\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "X_encoded = X_encoded.astype({col: int for col in X_encoded.select_dtypes('bool').columns})\n",
    "\n",
    "# Convert X_encoded to a PyTorch tensor\n",
    "X_tensor = torch.tensor(X_encoded.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['NObeyesdad'] = y['NObeyesdad'].astype('category').cat.codes\n",
    "\n",
    "# Convert to tensor\n",
    "y_tensor = torch.tensor(y['NObeyesdad'].to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = train_test_split(X_tensor, y_tensor, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNN(PyroModule):\n",
    "    def __init__(self, in_dim=1, out_dim=1, hid_dim=10, n_hid_layers=2, prior_scale=5.):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "\n",
    "        self.activation = nn.ReLU()  # could also be ReLU or LeakyReLU\n",
    "        assert in_dim > 0 and out_dim > 0 and hid_dim > 0 and n_hid_layers > 0  # make sure the dimensions are valid\n",
    "\n",
    "        # Define the layer sizes and the PyroModule layer list\n",
    "        self.layer_sizes = [in_dim] + n_hid_layers * [hid_dim] + [out_dim]\n",
    "        layer_list = [PyroModule[nn.Linear](self.layer_sizes[idx - 1], self.layer_sizes[idx]) for idx in\n",
    "                      range(1, len(self.layer_sizes))]\n",
    "        self.layers = PyroModule[torch.nn.ModuleList](layer_list)\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            layer.weight = PyroSample(dist.Normal(0., prior_scale * np.sqrt(2 / self.layer_sizes[layer_idx])).expand(\n",
    "                [self.layer_sizes[layer_idx + 1], self.layer_sizes[layer_idx]]).to_event(2))\n",
    "            layer.bias = PyroSample(dist.Normal(0., prior_scale).expand([self.layer_sizes[layer_idx + 1]]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = x.reshape(-1, self.in_dim)\n",
    "        x = self.activation(self.layers[0](x))  # input --> hidden\n",
    "        for layer in self.layers[1:-1]:\n",
    "            x = self.activation(layer(x))  # hidden --> hidden\n",
    "        x = self.layers[-1](x).squeeze()  # hidden --> output\n",
    "        x = torch.softmax(x, dim=1) # softmax activation\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Categorical(x), obs=y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 750/750 [14:45,  1.18s/it, step size=1.02e-03, acc. prob=0.929]\n"
     ]
    }
   ],
   "source": [
    "# Define Hamiltonian Monte Carlo (HMC) kernel\n",
    "# NUTS = \"No-U-Turn Sampler\" (https://arxiv.org/abs/1111.4246), gives HMC an adaptive step size\n",
    "model = BNN(in_dim = x_tr.shape[1], out_dim = 7, hid_dim=10, n_hid_layers=2, prior_scale=1)\n",
    "#in_dim=1, out_dim=1, hid_dim=10, n_hid_layers=5, prior_scale=5.\n",
    "\n",
    "nuts_kernel = NUTS(model, jit_compile=False)  # jit_compile=True is faster but requires PyTorch 1.6+\n",
    "\n",
    "# define model and data\n",
    "\n",
    "# define MCMC sampler\n",
    "nuts_kernel = NUTS(model, jit_compile=False)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=250, warmup_steps=500)\n",
    "mcmc.run(x_tr, y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['layers.0.bias', 'layers.0.weight', 'layers.1.bias', 'layers.1.weight', 'layers.2.bias', 'layers.2.weight']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 10, 23])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_samples = mcmc.get_samples()\n",
    "\n",
    "keys = list(post_samples.keys())\n",
    "print(keys)\n",
    "\n",
    "post_samples[keys[1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# # Save dictionary to a pickle file\n",
    "# with open('post_samples.pkl', 'wb') as f:\n",
    "#     pickle.dump(post_samples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary from the pickle file\n",
    "with open('post_samples.pkl', 'rb') as f:\n",
    "    loaded_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te = y_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'obs': tensor([[0, 4, 0,  ..., 5, 1, 3],\n",
       "         [0, 4, 0,  ..., 5, 1, 3],\n",
       "         [0, 4, 0,  ..., 5, 1, 3],\n",
       "         ...,\n",
       "         [0, 4, 0,  ..., 5, 1, 3],\n",
       "         [0, 4, 1,  ..., 5, 1, 3],\n",
       "         [0, 4, 0,  ..., 5, 1, 3]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictive(x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BNN.forward() got an unexpected keyword argument 'posterior_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_te\u001b[49m\u001b[43m,\u001b[49m\u001b[43mposterior_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloaded_samples\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m,:]\n",
      "File \u001b[1;32mc:\\Users\\peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyro\\nn\\module.py:520\u001b[0m, in \u001b[0;36mPyroModule.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pyro_context:\n\u001b[1;32m--> 520\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    522\u001b[0m         pyro\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate_poutine\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    523\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pyro_context\u001b[38;5;241m.\u001b[39mactive\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m _is_module_local_param_enabled()\n\u001b[0;32m    525\u001b[0m     ):\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_module_local_param_usage()\n",
      "File \u001b[1;32mc:\\Users\\peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: BNN.forward() got an unexpected keyword argument 'posterior_samples'"
     ]
    }
   ],
   "source": [
    "model(x_te,posterior_samples = loaded_samples)[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m predictive \u001b[38;5;241m=\u001b[39m Predictive(model\u001b[38;5;241m=\u001b[39mmodel, posterior_samples\u001b[38;5;241m=\u001b[39mloaded_samples)\n\u001b[0;32m      2\u001b[0m preds \u001b[38;5;241m=\u001b[39m predictive(x_te)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m mean_preds \u001b[38;5;241m=\u001b[39m \u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNLLLoss()\n\u001b[0;32m      5\u001b[0m loss(mean_preds, y_te)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Long"
     ]
    }
   ],
   "source": [
    "predictive = Predictive(model=model, posterior_samples=loaded_samples)\n",
    "preds = predictive(x_te)['obs']\n",
    "mean_preds = preds.mean(0)\n",
    "loss = nn.NLLLoss()\n",
    "loss(mean_preds, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9480, 0.0000, 0.8240,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0520, 0.0000, 0.1720,  ..., 0.1240, 0.9240, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0200],\n",
       "        ...,\n",
       "        [0.0000, 0.9920, 0.0000,  ..., 0.0000, 0.0000, 0.0320],\n",
       "        [0.0000, 0.0000, 0.0040,  ..., 0.8760, 0.0760, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_frequencies = torch.zeros(7, 634)\n",
    "for j in range(preds.shape[1]):\n",
    "    # Count the occurrences of each label in column j\n",
    "    label_counts = torch.bincount(preds[:, j], minlength=7)\n",
    "    # Calculate the relative frequency by dividing by the total number of predictions (50)\n",
    "    relative_frequencies[:, j] = label_counts.float() / preds.shape[0]\n",
    "\n",
    "relative_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_network_outputs(net_samples, input_data):\n",
    "    \"\"\"\n",
    "    Computes the outputs of the neural network for each of the 250 sampled weights and biases.\n",
    "\n",
    "    Parameters:\n",
    "    - loaded_samples (dict): Dictionary with keys like 'layers.0.bias', 'layers.0.weight', etc.\n",
    "    - input_data (torch.Tensor): Input data to feed into the network (shape [batch_size, input_dim]).\n",
    "\n",
    "    Returns:\n",
    "    - outputs (torch.Tensor): Network outputs for each sample (shape [250, batch_size, output_dim]).\n",
    "    \"\"\"\n",
    "    # Initialize list to store outputs for all 250 samples\n",
    "    outputs = []\n",
    "\n",
    "    # Loop over each sample\n",
    "    for i in range(loaded_samples['layers.0.bias'].shape[0]):  # 250 samples\n",
    "        # Sample weights and biases\n",
    "        layer_0_weight = loaded_samples['layers.0.weight'][i]  # Shape: [10, 23]\n",
    "        layer_0_bias = loaded_samples['layers.0.bias'][i]      # Shape: [10]\n",
    "        layer_1_weight = loaded_samples['layers.1.weight'][i]  # Shape: [23, X] (replace X with next layer size)\n",
    "        layer_1_bias = loaded_samples['layers.1.bias'][i]      # Shape: [X] (same as layer_1_weight's output size)\n",
    "        layer_2_weight = loaded_samples['layers.2.weight'][i]  # Shape: [X, output_dim]\n",
    "        layer_2_bias = loaded_samples['layers.2.bias'][i]      # Shape: [output_dim]\n",
    "\n",
    "        # Forward pass through the network for this sample\n",
    "        x = input_data\n",
    "        x = torch.matmul(x, layer_0_weight.T) + layer_0_bias  # First layer: shape [batch_size, 10]\n",
    "        x = torch.relu(x)  # Activation function (ReLU)\n",
    "        x = torch.matmul(x, layer_1_weight.T) + layer_1_bias  # Second layer\n",
    "        x = torch.relu(x)  # Activation function (ReLU)\n",
    "        x = torch.matmul(x, layer_2_weight.T) + layer_2_bias  # Third layer\n",
    "\n",
    "        # Store the output for this sample\n",
    "        outputs.append(x)\n",
    "\n",
    "    # Stack all outputs into a tensor (shape: [250, batch_size, output_dim])\n",
    "    outputs = torch.stack(outputs)\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = get_network_outputs(loaded_samples, x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
