{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 544, 'name': 'Estimation of Obesity Levels Based On Eating Habits and Physical Condition ', 'repository_url': 'https://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition', 'data_url': 'https://archive.ics.uci.edu/static/public/544/data.csv', 'abstract': 'This dataset include data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. ', 'area': 'Health and Medicine', 'tasks': ['Classification', 'Regression', 'Clustering'], 'characteristics': ['Multivariate'], 'num_instances': 2111, 'num_features': 16, 'feature_types': ['Integer'], 'demographics': ['Gender', 'Age'], 'target_col': ['NObeyesdad'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2019, 'last_updated': 'Tue Sep 10 2024', 'dataset_doi': '10.24432/C5H31Z', 'creators': [], 'intro_paper': {'ID': 358, 'type': 'NATIVE', 'title': 'Dataset for estimation of obesity levels based on eating habits and physical condition in individuals from Colombia, Peru and Mexico', 'authors': 'Fabio Mendoza Palechor, Alexis De la Hoz Manotas', 'venue': 'Data in Brief', 'year': 2019, 'journal': None, 'DOI': '10.1016/j.dib.2019.104344', 'URL': 'https://www.semanticscholar.org/paper/35b40bacd2ffa9370885b7a3004d88995fd1d011', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'This dataset include data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. The data contains 17 attributes and 2111 records, the records are labeled with the class variable NObesity (Obesity Level), that allows classification of the data using the values of Insufficient Weight, Normal Weight, Overweight Level I, Overweight Level II, Obesity Type I, Obesity Type II and Obesity Type III. 77% of the data was generated synthetically using the Weka tool and the SMOTE filter, 23% of the data was collected directly from users through a web platform.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Read the article (https://doi.org/10.1016/j.dib.2019.104344) to see the description of the attributes.', 'citation': None}}\n",
      "                              name     role         type demographic  \\\n",
      "0                           Gender  Feature  Categorical      Gender   \n",
      "1                              Age  Feature   Continuous         Age   \n",
      "2                           Height  Feature   Continuous        None   \n",
      "3                           Weight  Feature   Continuous        None   \n",
      "4   family_history_with_overweight  Feature       Binary        None   \n",
      "5                             FAVC  Feature       Binary        None   \n",
      "6                             FCVC  Feature      Integer        None   \n",
      "7                              NCP  Feature   Continuous        None   \n",
      "8                             CAEC  Feature  Categorical        None   \n",
      "9                            SMOKE  Feature       Binary        None   \n",
      "10                            CH2O  Feature   Continuous        None   \n",
      "11                             SCC  Feature       Binary        None   \n",
      "12                             FAF  Feature   Continuous        None   \n",
      "13                             TUE  Feature      Integer        None   \n",
      "14                            CALC  Feature  Categorical        None   \n",
      "15                          MTRANS  Feature  Categorical        None   \n",
      "16                      NObeyesdad   Target  Categorical        None   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                None  None             no  \n",
      "1                                                None  None             no  \n",
      "2                                                None  None             no  \n",
      "3                                                None  None             no  \n",
      "4   Has a family member suffered or suffers from o...  None             no  \n",
      "5            Do you eat high caloric food frequently?  None             no  \n",
      "6        Do you usually eat vegetables in your meals?  None             no  \n",
      "7              How many main meals do you have daily?  None             no  \n",
      "8                  Do you eat any food between meals?  None             no  \n",
      "9                                       Do you smoke?  None             no  \n",
      "10                 How much water do you drink daily?  None             no  \n",
      "11         Do you monitor the calories you eat daily?  None             no  \n",
      "12           How often do you have physical activity?  None             no  \n",
      "13  How much time do you use technological devices...  None             no  \n",
      "14                    How often do you drink alcohol?  None             no  \n",
      "15           Which transportation do you usually use?  None             no  \n",
      "16                                      Obesity level  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# fetch dataset \n",
    "estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition = fetch_ucirepo(id=544) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition.data.features \n",
    "y = estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition.data.targets.copy() \n",
    "  \n",
    "# metadata \n",
    "print(estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition.variables) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2111, 23])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC','CALC','MTRANS']\n",
    "# One-hot encode categorical features, dropping the first category\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "X_encoded = X_encoded.astype({col: int for col in X_encoded.select_dtypes('bool').columns})\n",
    "\n",
    "# Convert X_encoded to a PyTorch tensor\n",
    "X_tensor = torch.tensor(X_encoded.values, dtype=torch.float32)\n",
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['NObeyesdad'] = y['NObeyesdad'].astype('category').cat.codes\n",
    "\n",
    "# Convert to tensor\n",
    "y_tensor = torch.tensor(y['NObeyesdad'].to_numpy(), dtype=torch.long)\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(X_tensor, y_tensor, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullCovGaussianLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim, \n",
    "        output_dim, \n",
    "        prior_var = 0.8, \n",
    "        pre_trained_values = None   \n",
    "        ):\n",
    "        super(FullCovGaussianLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        # Prior parameters p(theta)\n",
    "\n",
    "        # If no prior parameters are given, set them to zero mean and given variance\n",
    "        if pre_trained_values is None:\n",
    "            self.mu_p = torch.zeros(input_dim * output_dim + output_dim)\n",
    "            self.chol_p = torch.eye(input_dim * output_dim + output_dim) * torch.sqrt(torch.tensor(prior_var))\n",
    "            \n",
    "        else:\n",
    "            # Extract pre-trained values\n",
    "            w_pre_mean = pre_trained_values[0]\n",
    "            w_pre_log_var = pre_trained_values[1]\n",
    "            b_pre_mean = pre_trained_values[2]\n",
    "            b_pre_log_var = pre_trained_values[3]\n",
    "\n",
    "            # Set priors to pre-trained values\n",
    "            self.mu_p = torch.cat([w_pre_mean, b_pre_mean], dim=0)\n",
    "            combined_log_vars = torch.cat([w_pre_log_var, b_pre_log_var], dim=0)\n",
    "            self.chol_p = torch.diag(torch.exp(combined_log_vars / 2))\n",
    "           \n",
    "\n",
    "         # Variational parameters q(theta)\n",
    "        self.mu_q = nn.Parameter(self.mu_p.clone(),requires_grad=True)\n",
    "        self.chol_q = nn.Parameter(self.chol_p.clone(),requires_grad=True)\n",
    "\n",
    "\n",
    "    def get_chol_q(self):\n",
    "        # Extract the lower triangular part of the parameter\n",
    "        L = torch.tril(self.chol_q, -1)        # Replace the diagonal with its softplus to ensure positivity\n",
    "        # L = torch.tanh(L_raw)  # Keeps values small, allows both signs\n",
    "\n",
    "        diag = torch.log1p(torch.exp(torch.diag(self.chol_q)))  # Softplus on original diagonal\n",
    "        return L + torch.diag(diag)\n",
    "\n",
    "    # the priors do not change so could be stored as attributes, but\n",
    "    # it feels cleaner to access them in the same way as the posteriors\n",
    "    def p(self):\n",
    "        \"\"\"Weight prior distribution\"\"\"\n",
    "        return torch.distributions.MultivariateNormal(self.mu_p, scale_tril=(self.chol_p* np.sqrt(2 / self.output_dim)))\n",
    "\n",
    "    # def q(self):\n",
    "    #     \"\"\"Variational weight posterior\"\"\"\n",
    "    #     return torch.distributions.MultivariateNormal(self.mu_q, scale_tril=self.chol_q)\n",
    "\n",
    "    def q(self):\n",
    "        \"\"\"Variational weight posterior with a valid Cholesky factor\"\"\"\n",
    "        valid_chol = self.get_chol_q()\n",
    "        return torch.distributions.MultivariateNormal(self.mu_q, scale_tril=valid_chol)  \n",
    "    \n",
    "      \n",
    "    # KL divergence KL[q||p] between two Gaussians\n",
    "    def kl_divergence(self):\n",
    "        return torch.distributions.kl_divergence(self.q(), self.p()).sum()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Propagates x through this layer by sampling weights from the posterior\"\"\"\n",
    "        assert (len(x.shape) == 3), \"x should be shape (num_samples, batch_size, input_dim).\"\n",
    "        assert x.shape[-1] == self.input_dim\n",
    "        num_samples = x.shape[0]\n",
    "\n",
    "        # Sample the parameters and detach them from the hyperparameters graph\n",
    "        param_sample = self.q().rsample((num_samples,))\n",
    "        weights = param_sample[:,:self.input_dim * self.output_dim]  # Shape: (num_samples, input_dim * output_dim)\n",
    "        weights = weights.view(num_samples, self.input_dim, self.output_dim)  # Reshape to (num_samples, input_dim, output_dim)\n",
    "\n",
    "        biases = param_sample[:,self.input_dim * self.output_dim:].unsqueeze(1)  # Shape: (output_dim, num_samples)\n",
    "\n",
    "        return x @ weights + biases\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerCovBNN(nn.Module):\n",
    "    \"\"\"BNN with layer-wise block diagonal covariance Gaussian distributions.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dims,\n",
    "        output_dim,\n",
    "        activation = nn.ReLU(),\n",
    "        noise_std=1.0,\n",
    "        prior_var = 0.8,\n",
    "        pre_trained_values = None\n",
    "    ):\n",
    "        super(LayerCovBNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        self.network = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) + 1):\n",
    "            # If no pre trained parameters are given, set them to zero mean and given variance\n",
    "            if pre_trained_values is None:\n",
    "                if i == 0:\n",
    "                    self.network.append(FullCovGaussianLayer(self.input_dim, self.hidden_dims[i], prior_var = prior_var))\n",
    "                    self.network.append(self.activation)\n",
    "                elif i == len(hidden_dims):\n",
    "                    self.network.append(\n",
    "                        FullCovGaussianLayer(self.hidden_dims[i - 1], self.output_dim, prior_var = prior_var)\n",
    "                    )\n",
    "                    self.network.append(torch.nn.Softmax(dim=2))\n",
    "\n",
    "                else:\n",
    "                    self.network.append(\n",
    "                        FullCovGaussianLayer(self.hidden_dims[i - 1], self.hidden_dims[i], prior_var = prior_var)\n",
    "                    )\n",
    "                    self.network.append(self.activation) \n",
    "\n",
    "            # Set to pre trained values\n",
    "            else:\n",
    "                if i == 0:\n",
    "                    self.network.append(FullCovGaussianLayer(self.input_dim, self.hidden_dims[i], pre_trained_values = pre_trained_values[i]))\n",
    "                    self.network.append(self.activation)\n",
    "                elif i == len(hidden_dims):\n",
    "                    self.network.append(\n",
    "                        FullCovGaussianLayer(self.hidden_dims[i - 1], self.output_dim, pre_trained_values = pre_trained_values[i])\n",
    "                    )\n",
    "                else:\n",
    "                    self.network.append(\n",
    "                        FullCovGaussianLayer(self.hidden_dims[i - 1], self.hidden_dims[i], pre_trained_values = pre_trained_values[i])\n",
    "                    )\n",
    "                    self.network.append(self.activation)\n",
    "\n",
    "    def forward(self, x, num_samples=1):\n",
    "        \"\"\"Propagate the inputs through the network using num_samples weights.\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): Inputs to the network.\n",
    "            num_samples (int, optional): Number of samples to use. Defaults to 1.\n",
    "        \"\"\"\n",
    "        assert len(x.shape) == 2, \"x.shape must be (batch_size, input_dim).\"\n",
    "        # Expand dimensions of x to (num_samples, batch_size, input_dim).\n",
    "        x = torch.unsqueeze(x, 0).repeat(num_samples, 1, 1)\n",
    "\n",
    "        # Propagate x through network\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, FullCovGaussianLayer):\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        assert len(x.shape) == 3, \"x.shape must be (num_samples, batch_size, output_dim)\"\n",
    "        assert x.shape[-1] == self.output_dim\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def ll(self, y_obs, y_pred, num_samples=1):\n",
    "        \"\"\"Computes the log likelihood of the outputs of self.forward(x)\"\"\"\n",
    "        l = torch.distributions.Categorical(probs=y_pred)\n",
    "        return l.log_prob(y_obs).mean(0).sum(0).squeeze()\n",
    "\n",
    "    def kl(self):\n",
    "        \"\"\"Computes the KL divergence between the approximate posterior and the prior for the network.\"\"\"\n",
    "        return sum([layer.kl_divergence() for layer in self.network if isinstance(layer, FullCovGaussianLayer)])\n",
    "    \n",
    "    \n",
    "    def loss(self, x, y, num_samples=1):\n",
    "        \"\"\"Computes the ELBO and returns its negative\"\"\"\n",
    "\n",
    "        y_pred = self.forward(x, num_samples=num_samples)\n",
    "        \n",
    "        exp_ll = self.ll(y, y_pred, num_samples=num_samples)\n",
    "        kl = self.kl()\n",
    "\n",
    "        return kl - exp_ll, exp_ll, kl\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerCovBNN(\n",
      "  (activation): ReLU()\n",
      "  (network): ModuleList(\n",
      "    (0): FullCovGaussianLayer()\n",
      "    (1): ReLU()\n",
      "    (2): FullCovGaussianLayer()\n",
      "    (3): ReLU()\n",
      "    (4): FullCovGaussianLayer()\n",
      "    (5): Softmax(dim=2)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bnn_model = LayerCovBNN(23, [10, 10], 7, noise_std=0.15)#, prior_var = 0.3)\n",
    "# find value of noise_std that works best by trial and error, but this is of course\n",
    "# inherently a bit contrived since we know we set input noise std to 0.15 in dataset generation\n",
    "print(bnn_model)\n",
    "\n",
    "opt = torch.optim.Adam(\n",
    "    bnn_model.parameters(),\n",
    "    lr = 1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8944, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.8944, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.8944,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.8944, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8944, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.8944]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_model.network[0].chol_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 408/1000 [00:15<00:22, 26.42it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter scale_tril (Tensor of shape (77, 77)) of distribution MultivariateNormal(loc: torch.Size([77]), scale_tril: torch.Size([77, 77])) to satisfy the constraint LowerCholesky(), but found invalid values:\ntensor([[ 0.8878,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.2003,  0.8153,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0370, -0.0595,  1.0429,  ...,  0.0000,  0.0000,  0.0000],\n        ...,\n        [ 0.0416, -0.1032,  0.0525,  ...,  0.7264,  0.0000,  0.0000],\n        [ 0.0628, -0.0787, -0.0675,  ...,  0.1157,  0.6392,  0.0000],\n        [-0.1032,  0.0542,  0.1158,  ..., -0.2638, -0.1406,  0.5811]],\n       grad_fn=<ExpandBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)):  \u001b[38;5;66;03m# epochs\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m     l, ll, kl \u001b[38;5;241m=\u001b[39m \u001b[43mbnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     tr_loss_evo\u001b[38;5;241m.\u001b[39mappend(l\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     15\u001b[0m     tr_ll_evo\u001b[38;5;241m.\u001b[39mappend(ll\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[1;32mIn[17], line 90\u001b[0m, in \u001b[0;36mLayerCovBNN.loss\u001b[1;34m(self, x, y, num_samples)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the ELBO and returns its negative\"\"\"\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     exp_ll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mll(y, y_pred, num_samples\u001b[38;5;241m=\u001b[39mnum_samples)\n\u001b[0;32m     93\u001b[0m     kl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkl()\n",
      "Cell \u001b[1;32mIn[17], line 68\u001b[0m, in \u001b[0;36mLayerCovBNN.forward\u001b[1;34m(self, x, num_samples)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, FullCovGaussianLayer):\n\u001b[1;32m---> 68\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n",
      "File \u001b[1;32mc:\\Users\\peter\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\peter\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[16], line 72\u001b[0m, in \u001b[0;36mFullCovGaussianLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     69\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Sample the parameters and detach them from the hyperparameters graph\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m param_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrsample((num_samples,))\n\u001b[0;32m     73\u001b[0m weights \u001b[38;5;241m=\u001b[39m param_sample[:,:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim]  \u001b[38;5;66;03m# Shape: (num_samples, input_dim * output_dim)\u001b[39;00m\n\u001b[0;32m     74\u001b[0m weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mview(num_samples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim)  \u001b[38;5;66;03m# Reshape to (num_samples, input_dim, output_dim)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 58\u001b[0m, in \u001b[0;36mFullCovGaussianLayer.q\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Variational weight posterior with a valid Cholesky factor\"\"\"\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# valid_chol = self.get_chol_q()\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmu_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_tril\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtril\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchol_q\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\distributions\\multivariate_normal.py:180\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[1;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m=\u001b[39m loc\u001b[38;5;241m.\u001b[39mexpand(batch_shape \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[0;32m    179\u001b[0m event_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale_tril \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m scale_tril\n",
      "File \u001b[1;32mc:\\Users\\peter\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\distributions\\distribution.py:71\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     69\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 71\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter scale_tril (Tensor of shape (77, 77)) of distribution MultivariateNormal(loc: torch.Size([77]), scale_tril: torch.Size([77, 77])) to satisfy the constraint LowerCholesky(), but found invalid values:\ntensor([[ 0.8878,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.2003,  0.8153,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0370, -0.0595,  1.0429,  ...,  0.0000,  0.0000,  0.0000],\n        ...,\n        [ 0.0416, -0.1032,  0.0525,  ...,  0.7264,  0.0000,  0.0000],\n        [ 0.0628, -0.0787, -0.0675,  ...,  0.1157,  0.6392,  0.0000],\n        [-0.1032,  0.0542,  0.1158,  ..., -0.2638, -0.1406,  0.5811]],\n       grad_fn=<ExpandBackward0>)"
     ]
    }
   ],
   "source": [
    "tr_loss_evo = []\n",
    "tr_ll_evo = []\n",
    "tr_kl_evo = []\n",
    "te_loss_evo = []\n",
    "\n",
    "tr_nll_loss_evo = []\n",
    "te_nll_loss_evo = []\n",
    "aux_loss = nn.NLLLoss()\n",
    "\n",
    "for epoch in tqdm(range(1000)):  # epochs\n",
    "    opt.zero_grad()\n",
    "\n",
    "    l, ll, kl = bnn_model.loss(x_tr, y_tr, num_samples = 1)\n",
    "    tr_loss_evo.append(l.item())\n",
    "    tr_ll_evo.append(ll.item())\n",
    "    tr_kl_evo.append(kl.item())\n",
    "    tr_nll_loss_evo.append(aux_loss(bnn_model(x_tr,num_samples=10).mean(0), y_tr).item())\n",
    "\n",
    "    te_loss_evo.append(bnn_model.loss(x_te, y_te)[0].item())\n",
    "    te_nll_loss_evo.append(aux_loss(bnn_model(x_te,num_samples=10).mean(0), y_te).item())\n",
    "\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "    \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "\n",
    "plt.plot(tr_loss_evo, label='Train dataset loss')\n",
    "plt.plot(te_loss_evo, label='Test dataset loss')\n",
    "plt.ylabel('ELBO loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(tr_ll_evo, label='Train dataset log-like evo')\n",
    "plt.ylabel('expected log likelihood')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.title('Expected Log Likelihood Evolution')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(tr_kl_evo, label='Train dataset kl evo')\n",
    "plt.ylabel('kl')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.title('KL Evolution')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(tr_nll_loss_evo, label='Train dataset')\n",
    "plt.plot(te_nll_loss_evo, label='Test dataset')\n",
    "plt.ylabel('NLL')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.title('MSE Evolution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 634, 7])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = bnn_model(x_te, num_samples = 1000)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0465, -0.0188, -0.0558, -0.0492, -0.0461, -0.0303, -0.0464, -0.0637,\n",
       "        -0.0364, -0.0966, -0.0819, -0.0097, -0.0325, -0.0581, -0.0224, -0.0096,\n",
       "        -0.0336, -0.0166, -0.0179, -0.0214, -0.0764, -0.0301, -0.0326, -0.1016,\n",
       "        -0.0331, -0.0189, -0.0152, -0.0870, -0.0357, -0.0302, -0.0381, -0.0235,\n",
       "        -0.0252, -0.0404, -0.0303, -0.0244, -0.0454, -0.0160, -0.0232, -0.0524,\n",
       "        -0.0657, -0.0196, -0.0388, -0.0383, -0.0350, -0.0166, -0.0261, -0.0115,\n",
       "        -0.0290, -0.0329, -0.0281, -0.0304, -0.0159, -0.0399, -0.0184, -0.0101,\n",
       "        -0.0264, -0.0125, -0.0342, -0.0418, -0.0116, -0.0143,  0.0004, -0.0232,\n",
       "        -0.0116, -0.0040, -0.0231, -0.0204, -0.0189, -0.0104, -0.0465, -0.0445,\n",
       "        -0.0492, -0.0663, -0.0286, -0.0361, -0.0727, -0.0497, -0.0220, -0.0490,\n",
       "        -0.0093, -0.0178, -0.0219, -0.0391, -0.0295, -0.0216, -0.0202, -0.0056,\n",
       "        -0.0235, -0.0109, -0.0435, -0.0278, -0.0150, -0.0346, -0.0160, -0.0221,\n",
       "         0.0011, -0.0265,  0.0036, -0.0413, -0.3203, -0.3103, -0.3253, -0.3257,\n",
       "        -0.3218, -0.3157, -0.3249, -0.3221, -0.3340, -0.3337],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_model.network[2].mu_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_preds = preds.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9453, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.NLLLoss()\n",
    "loss(torch.log(mean_preds),y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = mean_preds.argmax(dim=1)\n",
    "indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3748,  3.1310,  1.9544, -0.2291, -0.5085, -0.4909, -0.6243, -0.6867],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_model.network[0].mu_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_preds = preds.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-4.1884e-01, -4.7776e-01, -5.3141e-01, -4.4068e-01, -4.9450e-01,\n",
       "        -5.0225e-01, -3.8852e-01, -5.5507e-01, -4.8590e-01, -5.1330e-01,\n",
       "        -1.0245e-02, -1.2694e-02, -6.1323e-03, -1.7499e-03, -1.3763e-03,\n",
       "        -6.4404e-03, -2.1659e-03, -1.3864e-03, -3.9279e-03, -1.8517e-03,\n",
       "        -6.0320e-01, -7.1386e-01, -7.4161e-01, -6.7886e-01, -7.5077e-01,\n",
       "        -7.4513e-01, -7.5550e-01, -7.5444e-01, -7.6525e-01, -7.9469e-01,\n",
       "        -1.4323e-02, -2.0150e-02, -1.4527e-02, -3.9126e-03, -3.3805e-03,\n",
       "        -1.0646e-02, -3.8671e-03, -6.2496e-03, -5.8802e-03, -7.1817e-03,\n",
       "        -1.6366e-02, -2.3205e-02, -1.8341e-02, -4.6540e-03, -4.4864e-03,\n",
       "        -1.2535e-02, -4.0783e-03, -9.3975e-03, -7.8223e-03, -1.0243e-02,\n",
       "        -1.2298e-02, -1.5908e-02, -9.4683e-03, -2.3574e-03, -2.0758e-03,\n",
       "        -8.3673e-03, -2.8798e-03, -2.6698e-03, -4.7026e-03, -3.6455e-03,\n",
       "        -8.4970e-03, -4.8567e-03, -1.4550e-03, -7.7445e-04, -9.0807e-04,\n",
       "        -1.9591e-03, -1.2056e-03, -5.4292e-04, -2.0017e-03, -2.0171e-04,\n",
       "        -7.5536e-03,  5.0132e-07, -9.0113e-05, -3.5222e-04, -2.4109e-04,\n",
       "        -4.6546e-05,  1.0023e-04, -2.6730e-04, -2.4599e-04, -9.1540e-06,\n",
       "        -3.3175e-03,  1.1363e-03, -7.6827e-06,  3.9073e-04, -6.0724e-05,\n",
       "         3.1137e-04,  9.7218e-05,  6.0156e-06,  3.9309e-04, -3.7704e-05,\n",
       "        -8.5879e-03, -1.4759e-03, -2.7582e-04, -4.0853e-04, -4.4315e-04,\n",
       "        -9.2980e-04, -7.9483e-04, -2.9184e-04, -6.3422e-04, -6.4682e-05,\n",
       "        -8.0068e-03, -2.7634e-03, -4.2099e-04, -4.9720e-04, -5.2932e-04,\n",
       "        -1.2981e-03, -5.9896e-04, -3.0274e-04, -1.0113e-03, -5.1055e-05,\n",
       "        -5.6713e-05, -2.9067e-04, -9.7691e-07,  1.4723e-05, -1.4928e-04,\n",
       "         2.7983e-05, -1.1925e-05, -3.1074e-07,  9.2337e-06,  4.8434e-05,\n",
       "        -8.3078e-03, -1.9318e-03, -3.0887e-04, -4.2387e-04, -4.2342e-04,\n",
       "        -1.0868e-03, -6.9755e-04, -2.9333e-04, -8.0619e-04, -7.6222e-05,\n",
       "         4.3301e-08, -1.3439e-04, -7.6322e-07,  1.0368e-07, -2.6318e-11,\n",
       "        -3.5694e-05, -4.5689e-21, -4.5182e-09, -3.4011e-08,  1.3528e-08,\n",
       "        -1.6975e-05, -4.3727e-05, -2.8954e-05, -6.7704e-05, -2.8090e-04,\n",
       "        -3.0064e-05,  4.5523e-04, -9.2092e-05,  7.8398e-05, -1.2009e-04,\n",
       "         9.7868e-05,  2.9971e-04, -5.2838e-05, -4.6416e-05, -1.3064e-04,\n",
       "        -2.3559e-06,  1.0238e-04,  5.1202e-05,  6.8955e-05,  1.3151e-04,\n",
       "        -1.5735e-03,  1.1668e-04, -6.8304e-08,  3.3426e-05, -1.7701e-04,\n",
       "        -8.2697e-05,  5.2483e-06, -6.9747e-06,  1.4492e-05, -1.1337e-04,\n",
       "        -5.9298e-03, -8.7781e-05, -6.6705e-05, -6.8805e-04, -2.8341e-04,\n",
       "        -5.7812e-05, -1.1084e-04, -3.1702e-04, -1.8216e-05,  3.5908e-05,\n",
       "        -3.1522e-03,  3.9915e-05, -1.3488e-05,  3.7939e-04, -9.1890e-05,\n",
       "         4.2961e-05, -3.0482e-04,  3.0752e-05,  2.8633e-05, -1.3410e-05,\n",
       "        -7.1026e-08,  2.3338e-05,  1.2898e-07, -4.8585e-12, -2.6895e-12,\n",
       "         1.4026e-05,  1.2752e-10, -5.2344e-08,  2.9472e-06, -1.1071e-08,\n",
       "        -1.6893e-04, -1.4293e-05, -3.1006e-07, -8.7565e-05, -1.2659e-08,\n",
       "        -1.6803e-05,  1.0142e-04,  5.6073e-08, -2.2735e-05, -5.1143e-10,\n",
       "        -8.1959e-03, -1.4690e-03, -2.4398e-04, -4.2832e-04, -4.3646e-04,\n",
       "        -5.6430e-04,  2.0507e-05, -3.0006e-04, -7.4471e-04, -3.1070e-05,\n",
       "        -2.4878e-04,  1.1167e-05, -2.9318e-06,  4.9953e-06, -2.5383e-04,\n",
       "         2.4387e-06, -3.5586e-05,  5.1763e-07, -3.3940e-05, -2.0731e-05,\n",
       "        -7.4847e-03, -4.3296e-03, -8.3531e-04, -5.8297e-04, -6.3367e-04,\n",
       "        -2.0210e-03, -9.1793e-04, -3.4763e-04, -1.4175e-03, -1.4155e-04],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_model.network[0].mu_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, ll, kl = bnn_model.loss(x_tr, y_tr,num_samples = 1000)\n",
    "\n",
    "\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0., -0., -0., -0., -0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn_model.network[4].mu_q.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
