{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0          1        17.99         10.38          122.80     1001.0   \n",
       "1          1        20.57         17.77          132.90     1326.0   \n",
       "2          1        19.69         21.25          130.00     1203.0   \n",
       "3          1        11.42         20.38           77.58      386.1   \n",
       "4          1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset directly from the UCI repository\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "column_names = [\n",
    "    \"ID\", \"Diagnosis\",\n",
    "    \"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\",\n",
    "    \"compactness_mean\", \"concavity_mean\", \"concave points_mean\", \"symmetry_mean\", \"fractal_dimension_mean\",\n",
    "    \"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\", \"smoothness_se\",\n",
    "    \"compactness_se\", \"concavity_se\", \"concave points_se\", \"symmetry_se\", \"fractal_dimension_se\",\n",
    "    \"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\", \"smoothness_worst\",\n",
    "    \"compactness_worst\", \"concavity_worst\", \"concave points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Drop the ID column\n",
    "df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Convert diagnosis column to binary (M = 1, B = 0)\n",
    "df[\"Diagnosis\"] = df[\"Diagnosis\"].map({\"M\": 1, \"B\": 0})\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features (X) and target (y)\n",
    "X = df.drop(columns=['Diagnosis']).values\n",
    "y = df['Diagnosis'].values\n",
    "\n",
    "# Convert to torch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([398, 30])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_tr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[43mx_tr\u001b[49m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_tr' is not defined"
     ]
    }
   ],
   "source": [
    "x_tr = torch.FloatTensor(x_tr).unsqueeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanFieldLayer(nn.Module):\n",
    "    \"\"\"Represents a mean-field Gaussian distribution over each layer of the network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, init_var=1e-3):\n",
    "        super(MeanFieldLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Prior parameters p(theta)\n",
    "        self.w_mu_p = torch.zeros(input_dim, output_dim)\n",
    "        self.w_log_var_p = torch.zeros(input_dim, output_dim)\n",
    "        self.b_mu_p = torch.zeros(output_dim)\n",
    "        self.b_log_var_p = torch.zeros(output_dim)\n",
    "\n",
    "        # Variational parameters q(theta)\n",
    "        self.w_mu_q = nn.Parameter(torch.zeros(input_dim, output_dim), requires_grad=True)\n",
    "        self.w_log_var_q = nn.Parameter(\n",
    "            torch.ones(input_dim, output_dim) * torch.log(torch.tensor(init_var)), requires_grad=True\n",
    "        )  \n",
    "        self.b_mu_q = nn.Parameter(torch.zeros(output_dim), requires_grad=True)\n",
    "        self.b_log_var_q = nn.Parameter(\n",
    "            torch.ones(output_dim) * torch.log(torch.tensor(init_var)), requires_grad=True\n",
    "        )\n",
    "\n",
    "    # the priors do not change so could be stored as attributes, but\n",
    "    # it feels cleaner to access them in the same way as the posteriors\n",
    "    def p_w(self):\n",
    "        \"\"\"weight prior distribution\"\"\"\n",
    "        return torch.distributions.Normal(self.w_mu_p, (0.5 * self.w_log_var_p).exp())\n",
    "\n",
    "    def p_b(self):\n",
    "        \"\"\"bias prior distribution\"\"\"\n",
    "        return torch.distributions.Normal(self.b_mu_p, (0.5 * self.b_log_var_p).exp())\n",
    "\n",
    "    def q_w(self):\n",
    "        \"\"\"variational weight posterior\"\"\"\n",
    "        return torch.distributions.Normal(self.w_mu_q, (0.5 * self.w_log_var_q).exp())\n",
    "\n",
    "    def q_b(self):\n",
    "        \"\"\"variational bias posterior\"\"\"\n",
    "        return torch.distributions.Normal(self.b_mu_q, (0.5 * self.b_log_var_q).exp())\n",
    "\n",
    "    def kl(self):\n",
    "        weight_kl = torch.distributions.kl.kl_divergence(self.q_w(), self.p_w()).sum() \n",
    "        bias_kl = torch.distributions.kl.kl_divergence(self.q_b(), self.p_b()).sum()\n",
    "        return weight_kl + bias_kl\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Propagates x through this layer by sampling weights from the posterior\"\"\"\n",
    "        assert (len(x.shape) == 3), \"x should be shape (num_samples, batch_size, input_dim).\"\n",
    "        assert x.shape[-1] == self.input_dim\n",
    "\n",
    "        num_samples = x.shape[0]\n",
    "        # rsample carries out reparameterisation trick for us\n",
    "        weights = self.q_w().rsample((num_samples,))  # (num_samples, input_dim, output_dim).\n",
    "        biases = self.q_b().rsample((num_samples,)).unsqueeze(1)  # (num_samples, batch_size, output_dim)\n",
    "        return x @ weights + biases # (num_samples, batch_size, output_dim).\n",
    "\n",
    "\n",
    "\n",
    "class MeanFieldBNN(nn.Module):\n",
    "    \"\"\"Mean-field variational inference BNN.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dims,\n",
    "        output_dim,\n",
    "        # activation=nn.LeakyReLU(negative_slope=0.01),\n",
    "        activation=nn.ReLU(),\n",
    "        noise_std=1.0,\n",
    "    ):\n",
    "        super(MeanFieldBNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.noise_covar = torch.eye(output_dim)*noise_std\n",
    "\n",
    "        self.network = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) + 1):\n",
    "            if i == 0:\n",
    "                self.network.append(MeanFieldLayer(self.input_dim, self.hidden_dims[i]))\n",
    "                self.network.append(self.activation)\n",
    "            elif i == len(hidden_dims):\n",
    "                self.network.append(\n",
    "                    MeanFieldLayer(self.hidden_dims[i - 1], self.output_dim)\n",
    "                )\n",
    "                self.network.append(torch.nn.Sigmoid())\n",
    "\n",
    "            else:\n",
    "                self.network.append(\n",
    "                    MeanFieldLayer(self.hidden_dims[i - 1], self.hidden_dims[i])\n",
    "                )\n",
    "                self.network.append(self.activation) \n",
    "\n",
    "    def forward(self, x, num_samples=1):\n",
    "        \"\"\"Propagate the inputs through the network using num_samples weights.\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): Inputs to the network.\n",
    "            num_samples (int, optional): Number of samples to use. Defaults to 1.\n",
    "        \"\"\"\n",
    "        assert len(x.shape) == 2, \"x.shape must be (batch_size, input_dim).\"\n",
    "\n",
    "        # Expand dimensions of x to (num_samples, batch_size, input_dim).\n",
    "        x = torch.unsqueeze(x, 0).repeat(num_samples, 1, 1)\n",
    "\n",
    "        # Propagate x through network\n",
    "        for layer in self.network:\n",
    "            x = layer(x)\n",
    "\n",
    "        assert len(x.shape) == 3, \"x.shape must be (num_samples, batch_size, output_dim)\"\n",
    "        assert x.shape[-1] == self.output_dim\n",
    "\n",
    "        return x\n",
    "\n",
    "    def ll(self, y_obs, y_pred, num_samples=1):\n",
    "        \"\"\"Computes the log likelihood of the outputs of self.forward(x) using a Bernoulli likelihood.\"\"\"\n",
    "\n",
    "        y_pred = y_pred.squeeze(0)\n",
    "\n",
    "        bernoulli = torch.distributions.Bernoulli(probs=y_pred)\n",
    "\n",
    "        ll_tensor = bernoulli.log_prob(y_obs).mean(0) \n",
    "\n",
    "        return ll_tensor.squeeze()\n",
    "\n",
    "\n",
    "    def kl(self):\n",
    "        \"\"\"Computes the KL divergence between the approximate posterior and the prior for the network.\"\"\"\n",
    "        return sum([layer.kl() for layer in self.network if isinstance(layer, MeanFieldLayer)])\n",
    "\n",
    "    def loss(self, x, y, num_samples=1):\n",
    "        \"\"\"Computes the ELBO and returns its negative\"\"\"\n",
    "\n",
    "        y_pred = self.forward(x, num_samples=num_samples)\n",
    "        exp_ll = self.ll(y, y_pred, num_samples=num_samples)\n",
    "        kl = self.kl()\n",
    "        print(kl, exp_ll.shape)\n",
    "        return kl - exp_ll, exp_ll, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeanFieldBNN(\n",
      "  (activation): ReLU()\n",
      "  (network): ModuleList(\n",
      "    (0): MeanFieldLayer()\n",
      "    (1): ReLU()\n",
      "    (2): MeanFieldLayer()\n",
      "    (3): ReLU()\n",
      "    (4): MeanFieldLayer()\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bnn_model = MeanFieldBNN(30, [20, 20], 1, noise_std=0.15)\n",
    "# find value of noise_std that works best by trial and error, but this is of course\n",
    "# inherently a bit contrived since we know we set input noise std to 0.15 in dataset generation\n",
    "print(bnn_model)\n",
    "\n",
    "opt = torch.optim.Adam(\n",
    "    bnn_model.parameters(),\n",
    "    lr = 1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3134.5947, grad_fn=<AddBackward0>) torch.Size([398])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 398 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m l, ll, kl \u001b[38;5;241m=\u001b[39m bnn_model\u001b[38;5;241m.\u001b[39mloss(x_tr, y_tr)\n\u001b[1;32m---> 14\u001b[0m tr_loss_evo\u001b[38;5;241m.\u001b[39mappend(\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     15\u001b[0m tr_ll_evo\u001b[38;5;241m.\u001b[39mappend(ll\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     16\u001b[0m tr_kl_evo\u001b[38;5;241m.\u001b[39mappend(kl\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a Tensor with 398 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "tr_loss_evo = []\n",
    "tr_ll_evo = []\n",
    "tr_kl_evo = []\n",
    "te_loss_evo = []\n",
    "\n",
    "tr_mse_loss_evo = []\n",
    "te_mse_loss_evo = []\n",
    "aux_loss = nn.MSELoss()\n",
    "\n",
    "for epoch in tqdm(range(1)):  # epochs\n",
    "    opt.zero_grad()\n",
    "\n",
    "    l, ll, kl = bnn_model.loss(x_tr, y_tr)\n",
    "    tr_loss_evo.append(l.item())\n",
    "    tr_ll_evo.append(ll.item())\n",
    "    tr_kl_evo.append(kl.item())\n",
    "    tr_mse_loss_evo.append(aux_loss(bnn_model(x_tr), y_tr.unsqueeze(0)).item())\n",
    "\n",
    "    te_loss_evo.append(bnn_model.loss(x_te, y_te)[0].item())\n",
    "    te_mse_loss_evo.append(aux_loss(bnn_model(x_te), y_te.unsqueeze(0)).item())\n",
    "\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "\n",
    "plt.plot(tr_loss_evo, label='Train dataset loss')\n",
    "plt.plot(te_loss_evo, label='Test dataset loss')\n",
    "plt.ylabel('ELBO loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(tr_ll_evo, label='Train dataset log-like evo')\n",
    "plt.ylabel('expected log likelihood')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.title('Expected Log Likelihood Evolution')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(tr_kl_evo, label='Train dataset kl evo')\n",
    "plt.ylabel('kl')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.title('KL Evolution')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(tr_mse_loss_evo, label='Train dataset')\n",
    "plt.plot(te_mse_loss_evo, label='Test dataset')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.title('MSE Evolution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
