{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn\n",
    "# !pip install tensorflow\n",
    "# !pip install pandas\n",
    "# !pip install torch\n",
    "# !pip install tensorflow_probability\n",
    "# !pip install tqdm\n",
    "# !pip install tf_keras\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fetch_openml(name=\"boston\", version=1, as_frame=False)\n",
    "X, y = data.data.astype(float), data.target\n",
    "\n",
    "x_train, x_test = torch.from_numpy(X[:350,:]).float(), torch.from_numpy(X[350:,:]).float()\n",
    "y_train, y_test = torch.from_numpy(y[:350]).float(), torch.from_numpy(y[350:]).float()\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanFieldLayer(nn.Module):\n",
    "    \"\"\"Represents a mean-field Gaussian distribution over each layer of the network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, init_var=1e-3):\n",
    "        super(MeanFieldLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Prior parameters p(theta)\n",
    "        self.w_mu_p = torch.zeros(input_dim, output_dim)\n",
    "        self.w_log_var_p = torch.zeros(input_dim, output_dim)\n",
    "        self.b_mu_p = torch.zeros(output_dim)\n",
    "        self.b_log_var_p = torch.zeros(output_dim)\n",
    "\n",
    "        # Variational parameters q(theta)\n",
    "        self.w_mu_q = nn.Parameter(torch.zeros(input_dim, output_dim), requires_grad=True)\n",
    "        self.w_log_var_q = nn.Parameter(\n",
    "            torch.ones(input_dim, output_dim) * torch.log(torch.tensor(init_var)), requires_grad=True\n",
    "        )  \n",
    "        self.b_mu_q = nn.Parameter(torch.zeros(output_dim), requires_grad=True)\n",
    "        self.b_log_var_q = nn.Parameter(\n",
    "            torch.ones(output_dim) * torch.log(torch.tensor(init_var)), requires_grad=True\n",
    "        )\n",
    "\n",
    "    # the priors do not change so could be stored as attributes, but\n",
    "    # it feels cleaner to access them in the same way as the posteriors\n",
    "    def p_w(self):\n",
    "        \"\"\"weight prior distribution\"\"\"\n",
    "        return torch.distributions.Normal(self.w_mu_p, (0.5 * self.w_log_var_p).exp())\n",
    "\n",
    "    def p_b(self):\n",
    "        \"\"\"bias prior distribution\"\"\"\n",
    "        return torch.distributions.Normal(self.b_mu_p, (0.5 * self.b_log_var_p).exp())\n",
    "\n",
    "    def q_w(self):\n",
    "        \"\"\"variational weight posterior\"\"\"\n",
    "        return torch.distributions.Normal(self.w_mu_q, (0.5 * self.w_log_var_q).exp())\n",
    "\n",
    "    def q_b(self):\n",
    "        \"\"\"variational bias posterior\"\"\"\n",
    "        return torch.distributions.Normal(self.b_mu_q, (0.5 * self.b_log_var_q).exp())\n",
    "\n",
    "    def kl(self):\n",
    "        weight_kl = torch.distributions.kl.kl_divergence(self.q_w(), self.p_w()).sum() \n",
    "        bias_kl = torch.distributions.kl.kl_divergence(self.q_b(), self.p_b()).sum()\n",
    "        return weight_kl + bias_kl\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Propagates x through this layer by sampling weights from the posterior\"\"\"\n",
    "        assert (len(x.shape) == 3), \"x should be shape (num_samples, batch_size, input_dim).\"\n",
    "        assert x.shape[-1] == self.input_dim\n",
    "\n",
    "        num_samples = x.shape[0]\n",
    "        # rsample carries out reparameterisation trick for us\n",
    "        weights = self.q_w().rsample((num_samples,))  # (num_samples, input_dim, output_dim).\n",
    "        biases = self.q_b().rsample((num_samples,)).unsqueeze(1)  # (num_samples, batch_size, output_dim)\n",
    "        return x @ weights + biases # (num_samples, batch_size, output_dim).\n",
    "\n",
    "\n",
    "\n",
    "class MeanFieldBNN(nn.Module):\n",
    "    \"\"\"Mean-field variational inference BNN.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dims,\n",
    "        output_dim,\n",
    "        activation=nn.LeakyReLU(negative_slope=0.1),\n",
    "        noise_std=1.0,\n",
    "    ):\n",
    "        super(MeanFieldBNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.noise_covar = torch.eye(output_dim)*noise_std\n",
    "\n",
    "        self.network = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) + 1):\n",
    "            if i == 0:\n",
    "                self.network.append(MeanFieldLayer(self.input_dim, self.hidden_dims[i]))\n",
    "                self.network.append(self.activation)\n",
    "            elif i == len(hidden_dims):\n",
    "                self.network.append(\n",
    "                    MeanFieldLayer(self.hidden_dims[i - 1], self.output_dim)\n",
    "                )\n",
    "            else:\n",
    "                self.network.append(\n",
    "                    MeanFieldLayer(self.hidden_dims[i - 1], self.hidden_dims[i])\n",
    "                )\n",
    "                self.network.append(self.activation) \n",
    "\n",
    "    def forward(self, x, num_samples=1):\n",
    "        \"\"\"Propagate the inputs through the network using num_samples weights.\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): Inputs to the network.\n",
    "            num_samples (int, optional): Number of samples to use. Defaults to 1.\n",
    "        \"\"\"\n",
    "        assert len(x.shape) == 2, \"x.shape must be (batch_size, input_dim).\"\n",
    "\n",
    "        # Expand dimensions of x to (num_samples, batch_size, input_dim).\n",
    "        x = torch.unsqueeze(x, 0).repeat(num_samples, 1, 1)\n",
    "\n",
    "        # Propagate x through network\n",
    "        for layer in self.network:\n",
    "            x = layer(x)\n",
    "\n",
    "        assert len(x.shape) == 3, \"x.shape must be (num_samples, batch_size, output_dim)\"\n",
    "        assert x.shape[-1] == self.output_dim\n",
    "\n",
    "        return x\n",
    "\n",
    "    def ll(self, y_obs, y_pred, num_samples=1):\n",
    "        \"\"\"Computes the log likelihood of the outputs of self.forward(x)\"\"\"\n",
    "\n",
    "        ll_tensor = torch.tensor(0)\n",
    "        for i, y in enumerate(y_pred.squeeze(0)):\n",
    "            l = torch.distributions.multivariate_normal.MultivariateNormal(y, self.noise_covar)\n",
    "            ll_tensor = ll_tensor + l.log_prob(y_obs[i].repeat(num_samples,1,1)).mean(0)\n",
    "        \n",
    "        return ll_tensor.squeeze()\n",
    "\n",
    "    def kl(self):\n",
    "        \"\"\"Computes the KL divergence between the approximate posterior and the prior for the network.\"\"\"\n",
    "        return sum([layer.kl() for layer in self.network if isinstance(layer, MeanFieldLayer)])\n",
    "\n",
    "    def loss(self, x, y, num_samples=1):\n",
    "        \"\"\"Computes the ELBO and returns its negative\"\"\"\n",
    "\n",
    "        y_pred = self.forward(x, num_samples=num_samples)\n",
    "        exp_ll = self.ll(y, y_pred, num_samples=num_samples)\n",
    "        kl = self.kl()\n",
    "\n",
    "        return kl - exp_ll, exp_ll, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeanFieldBNN(\n",
      "  (activation): LeakyReLU(negative_slope=0.1)\n",
      "  (network): ModuleList(\n",
      "    (0): MeanFieldLayer()\n",
      "    (1): LeakyReLU(negative_slope=0.1)\n",
      "    (2): MeanFieldLayer()\n",
      "    (3): LeakyReLU(negative_slope=0.1)\n",
      "    (4): MeanFieldLayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bnn_model = MeanFieldBNN(13, [4, 4], 1, noise_std=1)\n",
    "print(bnn_model)\n",
    "\n",
    "opt = torch.optim.Adam(\n",
    "    bnn_model.parameters(),\n",
    "    lr = 1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 17.66it/s]\n",
      "/Users/patrickgunn/Documents/Documents - iCloud Drive/Uni Fourth Year/Full Year/Group Project/Bayesian-Neural-Network-Project/.venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([156])) that is different to the input size (torch.Size([1, 156, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(78.1715, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):  # epochs\n",
    "    opt.zero_grad()\n",
    "\n",
    "    l, ll, kl = bnn_model.loss(x_train, y_train)\n",
    "\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "\n",
    "preds = bnn_model(x_test)\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "mse(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
